import pandas as pd
import numpy as np
import glob
import xgboost as xgb
import os

# –î–æ–±–∞–≤–∏–ª confusion_matrix, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ –≤ –ø—Ä–æ—à–ª–æ–º –∑–∞–ø—É—Å–∫–µ
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
# –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—É—Ç–æ—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–∏—â–µ–º –≤–æ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–∫–∞—Ö)
path_sensors_pattern = 'data/**/*—Å—É—Ç–∫–∏*.xlsx'

path_failures_list = [
    'data/30/–¢–ù–°30 2023.xlsx',
    'data/30/–¢–ù–°30 2024.xlsx',
    'data/30/–¢–ù–°30 2025.xlsx',
    'data/16/–¢–ù–°16 2023.xlsx',
    'data/16/–¢–ù–°16 2024.xlsx',
    'data/16/–¢–ù–°16 2025.xlsx'
]

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –°–ï–ù–°–û–†–û–í (–°–£–¢–ö–ò) ---
print(f"üîç –ò—â–µ–º —Ñ–∞–π–ª—ã –ø–æ –º–∞—Å–∫–µ: {path_sensors_pattern}")
all_files = glob.glob(path_sensors_pattern, recursive=True)
print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")

df_list = []
count = 0

for filename in all_files:
    try:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª. skiprows=6 –æ–±—ã—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –∏ –¥–ª—è —Å—É—Ç–æ—á–Ω—ã—Ö
        temp = pd.read_excel(filename, skiprows=6, engine='openpyxl')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–∏–Ω–¥–µ–∫—Å—ã –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å—É—Ç–æ—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤)
        # 0:Date, 1:P1, 2:T1, 4:V1, 6:P2, 8:T2, 9:V2, 10:Q
        temp = temp.iloc[:, [0, 1, 2, 4, 6, 8, 9, 10]]
        temp.columns = ['datetime', 'p1', 't1', 'v1', 'p2', 't2', 'v2', 'q_heat']
        
        # –ß–∏—Å—Ç–∏–º –¥–∞—Ç—É
        temp['datetime'] = pd.to_datetime(temp['datetime'], dayfirst=True, errors='coerce')
        temp = temp.dropna(subset=['datetime'])
        
        df_list.append(temp)
        
        # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 100 —Ñ–∞–π–ª–æ–≤
        count += 1
        if count % 100 == 0:
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {count} —Ñ–∞–π–ª–æ–≤...")
            
    except Exception as e:
        # –ú–æ–∂–Ω–æ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –≤–∏–¥–µ—Ç—å –æ—à–∏–±–∫–∏ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ñ–∞–π–ª–∞–º
        # print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
        continue

if not df_list:
    raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")

print("–°–∫–ª–µ–π–∫–∞ —Ç–∞–±–ª–∏—Ü...")
sensors = pd.concat(df_list, ignore_index=True)

# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –≤–∞–∂–Ω–∞ –¥–ª—è —Å—É—Ç–æ—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
sensors = sensors.sort_values('datetime').reset_index(drop=True)

# –ß–∏—Å–ª–∞
cols_num = ['p1', 't1', 'v1', 'p2', 't2', 'v2', 'q_heat']
for col in cols_num:
    sensors[col] = pd.to_numeric(sensors[col], errors='coerce')

sensors = sensors.dropna()
sensors['date_only'] = sensors['datetime'].dt.date

print(f"‚úÖ –°–µ–Ω—Å–æ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(sensors)}")

# --- 2. –ó–ê–ì–†–£–ó–ö–ê –ê–í–ê–†–ò–ô ---
print("–ó–∞–≥—Ä—É–∑–∫–∞ –∞–≤–∞—Ä–∏–π...")
fail_list = []
for f_path in path_failures_list:
    try:
        if f_path.endswith('.csv'):
            temp_fail = pd.read_csv(f_path)
        else:
            temp_fail = pd.read_excel(f_path, engine='openpyxl')
        fail_list.append(temp_fail)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∞–≤–∞—Ä–∏–π {f_path}: {e}")

if fail_list:
    failures = pd.concat(fail_list, ignore_index=True)
    failures = failures.dropna(subset=['–î–∞—Ç–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è'])
    failures['date_fail'] = pd.to_datetime(failures['–î–∞—Ç–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è'], dayfirst=True, errors='coerce').dt.date
    failures = failures.dropna(subset=['date_fail'])
    print(f"–ê–≤–∞—Ä–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(failures)}")
else:
    print("‚ö†Ô∏è –ê–≤–∞—Ä–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.")
    failures = pd.DataFrame(columns=['date_fail'])

# --- 3. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (Feature Engineering) ---
sensors['target'] = 0
LOOK_AHEAD = 2  # –û–∫–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–¥–Ω–µ–π –¥–æ –∞–≤–∞—Ä–∏–∏)

# –†–∞–∑–º–µ—Ç–∫–∞ —Ç–∞—Ä–≥–µ—Ç–∞
if not failures.empty:
    for f_date in failures['date_fail']:
        start_danger = f_date - pd.Timedelta(days=LOOK_AHEAD)
        mask = (sensors['date_only'] >= start_danger) & (sensors['date_only'] <= f_date)
        sensors.loc[mask, 'target'] = 1

# –§–∏–∑–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
sensors['delta_p'] = sensors['p1'] - sensors['p2']
sensors['delta_t'] = sensors['t1'] - sensors['t2']
sensors['v_diff'] = sensors['v1'] - sensors['v2']

# Rolling (–°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ)
# –î–ª—è —Å—É—Ç–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—á–∞—Å–æ–≤—ã—Ö) –æ–∫–Ω–æ 24 - —ç—Ç–æ —Å—É—Ç–∫–∏
ROLLING_WINDOW = 24 
sensors['p1_mean_24h'] = sensors['p1'].rolling(window=ROLLING_WINDOW).mean()
sensors['p1_std_24h'] = sensors['p1'].rolling(window=ROLLING_WINDOW).std()
sensors['q_heat_mean_24h'] = sensors['q_heat'].rolling(window=ROLLING_WINDOW).mean()

df_final = sensors.dropna().drop(columns=['date_only'])

# –õ–∞–≥–∏ (—Å–¥–≤–∏–≥–∏ –Ω–∞–∑–∞–¥ –≤–æ –≤—Ä–µ–º–µ–Ω–∏)
LAG_WINDOW = 6
features_to_lag = ['delta_p', 'v_diff', 'delta_t', 'q_heat_mean_24h', 'p1_mean_24h', 'p1_std_24h']

for col in features_to_lag:
    for lag in range(1, LAG_WINDOW + 1):
        # –°–¥–≤–∏–≥ –Ω–∞ 1 —á–∞—Å, 2 —á–∞—Å–∞ –∏ —Ç.–¥.
        df_final[f'{col}_lag_{lag}h'] = df_final[col].shift(lag)

df_final = df_final.dropna()

# ... (—Ç–≤–æ–π –∫–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏—è –ª–∞–≥–æ–≤ –≤—ã—à–µ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

# --- –î–û–ë–ê–í–õ–ï–ù–ò–ï –°–ï–ó–û–ù–ù–û–°–¢–ò (–ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï) ---
print("üìÜ –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...")
df_final['month'] = df_final['datetime'].dt.month
df_final['hour'] = df_final['datetime'].dt.hour
# –û—Ç–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–∑–æ–Ω (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å –æ–∫—Ç—è–±—Ä—è –ø–æ –∞–ø—Ä–µ–ª—å)
df_final['is_heating_season'] = df_final['month'].isin([10, 11, 12, 1, 2, 3, 4]).astype(int)

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π
# –ë–µ—Ä–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ + –Ω–æ–≤—ã–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ
cols_to_drop = ['datetime', 'target']
feature_cols = [c for c in df_final.columns if c not in cols_to_drop]

print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({len(feature_cols)}): {feature_cols}")

# --- –ß–ï–°–¢–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø (TIME SERIES SPLIT) ---

print("\n‚è±Ô∏è –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏...")
df_final = df_final.sort_values('datetime')

# –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20% –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞ (–ë—É–¥—É—â–µ–µ)
split_idx = int(len(df_final) * 0.80)

X = df_final[feature_cols]
y = df_final['target']

X_train = X.iloc[:split_idx]
y_train = y.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_test = y.iloc[split_idx:]

print(f"–û–±—É—á–µ–Ω–∏–µ: {df_final.iloc[0]['datetime']} -> {df_final.iloc[split_idx]['datetime']}")
print(f"–¢–µ—Å—Ç:     {df_final.iloc[split_idx]['datetime']} -> {df_final.iloc[-1]['datetime']}")

# --- –û–ë–£–ß–ï–ù–ò–ï –° –ó–ê–©–ò–¢–û–ô –û–¢ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø ---
ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)

model = xgb.XGBClassifier(
    n_estimators=2000,       # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
    learning_rate=0.01,      # –ú–µ–Ω—å—à–µ —à–∞–≥ (—É—á–∏–º—Å—è –º–µ–¥–ª–µ–Ω–Ω–µ–µ –∏ –∞–∫–∫—É—Ä–∞—Ç–Ω–µ–µ)
    max_depth=4,             # –ú–µ–Ω—å—à–µ –≥–ª—É–±–∏–Ω–∞ (–±—ã–ª–æ 6) - —á—Ç–æ–±—ã –Ω–µ –∑—É–±—Ä–∏—Ç—å —à—É–º
    subsample=0.7,           # –ë–µ—Ä–µ–º –Ω–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
    colsample_bytree=0.7,    # –ë–µ—Ä–µ–º –Ω–µ –≤—Å–µ —Ñ–∏—á–∏ —Å—Ä–∞–∑—É (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
    scale_pos_weight=ratio,
    eval_metric='auc',
    early_stopping_rounds=200, # –î–∞–µ–º –±–æ–ª—å—à–µ —à–∞–Ω—Å–æ–≤ –Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=100
)

# --- –†–ï–ó–£–õ–¨–¢–ê–¢–´ ---
probs = model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, probs)
print(f"\nüèÜ REAL ROC-AUC Score: {auc:.4f}")

# –ü–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–∞
from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y_test, probs)

# –°—Ç–∞—Ä–∞–µ–º—Å—è –¥–µ—Ä–∂–∞—Ç—å Recall —Ö–æ—Ç—è –±—ã 70%
target_recall = 0.70
try:
    # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥, –∫–æ—Ç–æ—Ä—ã–π –¥–∞–µ—Ç Recall >= 0.70
    valid_idxs = np.where(recalls[:-1] >= target_recall)[0]
    if len(valid_idxs) > 0:
        optimal_threshold = thresholds[valid_idxs[-1]]
    else:
        optimal_threshold = 0.5
except:
    optimal_threshold = 0.5

print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ (–¥–ª—è Recall >{target_recall*100}%): {optimal_threshold:.4f}")

y_pred = (probs > optimal_threshold).astype(int)
print(classification_report(y_test, y_pred))

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–µ–ø–µ—Ä—å —Ç—É—Ç –¥–æ–ª–∂–Ω—ã –ø–æ—è–≤–∏—Ç—å—Å—è month –∏–ª–∏ is_heating_season)
plt.figure(figsize=(10, 8))
xgb.plot_importance(model, max_num_features=15)
plt.title("–¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å —É—á–µ—Ç–æ–º —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏)")
plt.savefig('seasonal_importance.png')
print("–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'seasonal_importance.png'")