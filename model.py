import pandas as pd
import numpy as np
import glob
import xgboost as xgb
import os
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_curve, accuracy_score
import matplotlib.pyplot as plt

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
path_sensors_pattern = 'data/**/*—Å—É—Ç–∫–∏*.xlsx'

path_failures_list = [
    'data/30/–¢–ù–°30 2023.xlsx',
    'data/30/–¢–ù–°30 2024.xlsx',
    'data/30/–¢–ù–°30 2025.xlsx',
    'data/16/–¢–ù–°16 2023.xlsx',
    'data/16/–¢–ù–°16 2024.xlsx',
    'data/16/–¢–ù–°16 2025.xlsx'
]

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –°–ï–ù–°–û–†–û–í ---
print(f"üîç –ò—â–µ–º —Ñ–∞–π–ª—ã –ø–æ –º–∞—Å–∫–µ: {path_sensors_pattern}")
all_files = glob.glob(path_sensors_pattern, recursive=True)
print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")

df_list = []
count = 0

for filename in all_files:
    try:
        temp = pd.read_excel(filename, skiprows=6, engine='openpyxl')
        temp = temp.iloc[:, [0, 1, 2, 4, 6, 8, 9, 10]]
        temp.columns = ['datetime', 'p1', 't1', 'v1', 'p2', 't2', 'v2', 'q_heat']
        temp['datetime'] = pd.to_datetime(temp['datetime'], dayfirst=True, errors='coerce')
        temp = temp.dropna(subset=['datetime'])
        df_list.append(temp)
        count += 1
        if count % 200 == 0:
            print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {count} —Ñ–∞–π–ª–æ–≤...")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
        continue

if not df_list:
    raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞!")

print("–°–∫–ª–µ–π–∫–∞ —Ç–∞–±–ª–∏—Ü...")
sensors = pd.concat(df_list, ignore_index=True)
sensors = sensors.sort_values('datetime').reset_index(drop=True)

for col in ['p1', 't1', 'v1', 'p2', 't2', 'v2', 'q_heat']:
    sensors[col] = pd.to_numeric(sensors[col], errors='coerce')

sensors = sensors.dropna()
sensors['date_only'] = sensors['datetime'].dt.date
print(f"‚úÖ –°–µ–Ω—Å–æ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(sensors)}")

# --- 2. –ó–ê–ì–†–£–ó–ö–ê –ê–í–ê–†–ò–ô ---
print("–ó–∞–≥—Ä—É–∑–∫–∞ –∞–≤–∞—Ä–∏–π...")
fail_list = []
for f_path in path_failures_list:
    try:
        if f_path.endswith('.csv'):
            temp_fail = pd.read_csv(f_path)
        else:
            temp_fail = pd.read_excel(f_path, engine='openpyxl')
        fail_list.append(temp_fail)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∞–≤–∞—Ä–∏–π {f_path}: {e}")

if fail_list:
    failures = pd.concat(fail_list, ignore_index=True)
    failures = failures.dropna(subset=['–î–∞—Ç–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è'])
    failures['date_fail'] = pd.to_datetime(failures['–î–∞—Ç–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è'], dayfirst=True, errors='coerce').dt.date
    failures = failures.dropna(subset=['date_fail'])
    print(f"–ê–≤–∞—Ä–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(failures)}")
else:
    failures = pd.DataFrame(columns=['date_fail'])

# --- 3. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---
sensors['target'] = 0
LOOK_AHEAD = 3  # <--- –ò–ó–ú–ï–ù–ò–õ–ò –ù–ê 3 –î–ù–Ø (–ª–æ–≤–∏–º –∑–∞—Ä–∞–Ω–µ–µ)

if not failures.empty:
    for f_date in failures['date_fail']:
        start_danger = f_date - pd.Timedelta(days=LOOK_AHEAD)
        mask = (sensors['date_only'] >= start_danger) & (sensors['date_only'] <= f_date)
        sensors.loc[mask, 'target'] = 1

sensors['delta_p'] = sensors['p1'] - sensors['p2']
sensors['delta_t'] = sensors['t1'] - sensors['t2']
sensors['v_diff'] = sensors['v1'] - sensors['v2']

ROLLING_WINDOW = 24
sensors['p1_mean_24h'] = sensors['p1'].rolling(window=ROLLING_WINDOW).mean()
sensors['p1_std_24h'] = sensors['p1'].rolling(window=ROLLING_WINDOW).std()
sensors['q_heat_mean_24h'] = sensors['q_heat'].rolling(window=ROLLING_WINDOW).mean()

df_final = sensors.dropna().drop(columns=['date_only'])

LAG_WINDOW = 6
features_to_lag = ['delta_p', 'v_diff', 'delta_t', 'q_heat_mean_24h', 'p1_mean_24h', 'p1_std_24h']

for col in features_to_lag:
    for lag in range(1, LAG_WINDOW + 1):
        df_final[f'{col}_lag_{lag}h'] = df_final[col].shift(lag)

df_final = df_final.dropna()

print("üìÜ –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å...")
df_final['month'] = df_final['datetime'].dt.month
df_final['hour'] = df_final['datetime'].dt.hour
df_final['is_heating_season'] = df_final['month'].isin([10, 11, 12, 1, 2, 3, 4]).astype(int)

cols_to_drop = ['datetime', 'target']
feature_cols = [c for c in df_final.columns if c not in cols_to_drop]

# --- 4. –í–ê–õ–ò–î–ê–¶–ò–Ø ---
print("\n‚è±Ô∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (Train: –ü—Ä–æ—à–ª–æ–µ, Test: –ë—É–¥—É—â–µ–µ)...")
df_final = df_final.sort_values('datetime')
split_idx = int(len(df_final) * 0.80)

X = df_final[feature_cols]
y = df_final['target']

X_train = X.iloc[:split_idx]
y_train = y.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_test = y.iloc[split_idx:]

print(f"–û–±—É—á–µ–Ω–∏–µ: {df_final.iloc[0]['datetime']} -> {df_final.iloc[split_idx]['datetime']}")
print(f"–¢–µ—Å—Ç:     {df_final.iloc[split_idx]['datetime']} -> {df_final.iloc[-1]['datetime']}")

# --- 5. –û–ë–£–ß–ï–ù–ò–ï ---
ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)

model = xgb.XGBClassifier(
    n_estimators=2000,
    learning_rate=0.01,
    max_depth=4,
    subsample=0.7,
    colsample_bytree=0.7,
    scale_pos_weight=ratio,
    eval_metric='auc',
    early_stopping_rounds=200,
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=100
)

# --- 6. –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–ê–í–¢–û-–ü–û–†–û–ì) ---
probs = model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, probs)
print(f"\nüèÜ ROC-AUC Score: {auc:.4f}")

# --- –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–û–î–ë–û–† –ü–û–†–û–ì–ê ---
TARGET_RECALL = 0.75  # –¶–µ–ª—å: –ª–æ–≤–∏—Ç—å 75% –∞–≤–∞—Ä–∏–π
print(f"\nüéØ –ü–æ–¥–±–∏—Ä–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è Recall >= {TARGET_RECALL*100}%...")

precisions, recalls, thresholds = precision_recall_curve(y_test, probs)

# –ò—â–µ–º –∏–Ω–¥–µ–∫—Å, –≥–¥–µ Recall >= 0.75 (–∏–¥–µ–º —Å –∫–æ–Ω—Ü–∞, —Ç.–∫. thresholds —Ä–∞—Å—Ç—É—Ç)
valid_idxs = np.where(recalls[:-1] >= TARGET_RECALL)[0]
if len(valid_idxs) > 0:
    optimal_threshold = thresholds[valid_idxs[-1]]
else:
    optimal_threshold = 0.5  # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, —Å—Ç–∞–≤–∏–º –¥–µ—Ñ–æ–ª—Ç (–≤—Ä—è–¥ –ª–∏ —Å–ª—É—á–∏—Ç—Å—è)

print(f"‚úÖ –ü–æ—Ä–æ–≥ –Ω–∞–π–¥–µ–Ω: {optimal_threshold:.4f}")

# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
y_pred = (probs > optimal_threshold).astype(int)

print("\nüìã –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
print(classification_report(y_test, y_pred))

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
print("\nüìä –ò–¢–û–ì–ò (–≤ —à—Ç—É–∫–∞—Ö):")
print(f"üî• –ü–æ–π–º–∞–Ω–æ –∞–≤–∞—Ä–∏–π (TP): {tp}")
print(f"üôà –ü—Ä–æ–ø—É—â–µ–Ω–æ (FN):      {fn}")
print(f"üì¢ –õ–æ–∂–Ω—ã—Ö —Ç—Ä–µ–≤–æ–≥ (FP):  {fp}")
print(f"‚úîÔ∏è –¢–∏—à–∏–Ω–∞ (TN):         {tn}")

# --- 7. –°–û–•–†–ê–ù–ï–ù–ò–ï ---
model.save_model("xgboost_final.json")
with open("model_features.pkl", "wb") as f:
    pickle.dump(feature_cols, f)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º Excel —Å —Ä–∏—Å–∫–∞–º–∏
df_final['risk_score'] = model.predict_proba(X)[:, 1]
# –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
report_cols = ['datetime', 'risk_score', 'target', 'p1', 't1', 'v_diff'] 
final_report_cols = [c for c in report_cols if c in df_final.columns]
df_final[final_report_cols].to_excel('final_risk_report.xlsx', index=False)

print("\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'xgboost_final.json'")
print("üíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –≤ 'final_risk_report.xlsx'")